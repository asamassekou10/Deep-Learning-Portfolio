{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **AI Research Assistant: Web-Based Information Retrieval and Summarization Agent**\n",
        "\n",
        "Alhassane Samassekou\n",
        "\n",
        "ITAI2376\n",
        "\n",
        "May 7, 2025"
      ],
      "metadata": {
        "id": "e5Ex57n8-Wry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "This project implements an AI Research Assistant that helps users:\n",
        "- Research topics and gather information from the web\n",
        "- Summarize findings from multiple sources\n",
        "- Organize information into structured reports\n",
        "- Generate PDF reports with proper *citations*"
      ],
      "metadata": {
        "id": "Tyl3j22U_N6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The agent integrates:**\n",
        "- Web search capabilities through SerpAPI\n",
        "- Document processing for summarization\n",
        "- PDF generation for formatted reports\n",
        "- A user-friendly Gradio interface"
      ],
      "metadata": {
        "id": "MpVfk0Aqr5PD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Goals**\n",
        "\n",
        "Create an AI agent that performs web-based research on user queries\n",
        "Implement information retrieval and summarization from multiple sources\n",
        "Provide organized, cited research findings to users\n",
        "Build a simple but effective user interface for interaction\n",
        "\n",
        "# **Core Features**\n",
        "\n",
        "Web search using the SerpAPI integration\n",
        "Text analysis and key information extraction\n",
        "Research summarization with source attribution\n",
        "User-friendly interface through Gradio"
      ],
      "metadata": {
        "id": "pOTvuqMI_VMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing Required Dependencies**"
      ],
      "metadata": {
        "id": "BiNmAuWD_j5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai serpapi faiss-cpu langchain transformers nltk ipywidgets gradio fpdf tiktoken -q\n"
      ],
      "metadata": {
        "id": "pn5AwFgF_LtF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries & Set Up Environment Variables**"
      ],
      "metadata": {
        "id": "RQiTb9K2_umh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import gradio as gr\n",
        "from datetime import datetime\n",
        "import re\n",
        "from collections import Counter\n",
        "from fpdf import FPDF\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import traceback\n",
        "import json\n",
        "\n",
        "# Set SerpAPI key - replace with your own key if needed\n",
        "SERPAPI_KEY = \"fe2ca6b2a2dc5344ef141c367c982d65f4aecc34ebfd96fe85b963c0fd1e71c5\"\n",
        "os.environ['SERPAPI_API_KEY'] = SERPAPI_KEY\n",
        "\n",
        "# Session Memory - for future enhancement of conversation history\n",
        "SESSION_MEMORY = {}\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbq-u1X2_0n3",
        "outputId": "c3030a3d-faf8-4508-891c-b8d2cf89929d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment setup complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "This initial cell sets up our research assistant project by installing all the necessary Python packages and importing the required libraries. The code establishes our working environment by:\n",
        "\n",
        "Installing packages for API access (openai, serpapi), vector search (faiss-cpu), LLM frameworks (langchain), text processing (transformers, nltk), UI components (ipywidgets, gradio), and PDF generation (fpdf)\n",
        "Setting up the SerpAPI key that will be used for web searches\n",
        "Creating a session memory dictionary for potential future enhancements that could track conversation history"
      ],
      "metadata": {
        "id": "MQ4kjc6pwe0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Debug Utilities**"
      ],
      "metadata": {
        "id": "-8lq9s2-_3lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_api_response(response_text, limit=1000):\n",
        "    \"\"\"Helper function to debug API responses\"\"\"\n",
        "    if not response_text:\n",
        "        return \"Empty response\"\n",
        "    try:\n",
        "        # Try to parse as JSON for better formatting\n",
        "        data = json.loads(response_text)\n",
        "        return json.dumps(data, indent=2)[:limit] + \"...\" if len(json.dumps(data)) > limit else json.dumps(data, indent=2)\n",
        "    except:\n",
        "        # If it's not JSON, just return text\n",
        "        return response_text[:limit] + \"...\" if len(response_text) > limit else response_text"
      ],
      "metadata": {
        "id": "1OkKWNEL_647"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "This utility function helps debug API responses by formatting and truncating them to make them more readable. The function:\n",
        "\n",
        "Attempts to parse the response as JSON for better formatted output\n",
        "Truncates long responses to a specified limit (default: 1000 characters) to prevent overwhelming output\n",
        "Falls back to plain text formatting if the response isn't valid JSON\n",
        "\n",
        "This function is particularly useful for monitoring and troubleshooting the SerpAPI responses during development, making it easier to identify issues with the search results."
      ],
      "metadata": {
        "id": "2otJA_e_wnhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Google Search Function via SerpAPI**"
      ],
      "metadata": {
        "id": "qLYaLkEx_9Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_google(query):\n",
        "    \"\"\"Enhanced SerpAPI search with better debugging\"\"\"\n",
        "    # Mock results as absolute last resort\n",
        "    MOCK_RESULTS = [\n",
        "        (\"Ethical Issues in AI Healthcare\", \"https://example.com/1\", \"AI in healthcare raises concerns about privacy, bias, and accountability.\"),\n",
        "        (\"AI Ethics in Medicine\", \"https://example.com/2\", \"Key ethical issues include data security and informed consent.\")\n",
        "    ]\n",
        "\n",
        "    # Set up SerpAPI parameters\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERPAPI_KEY,\n",
        "        \"num\": 10,\n",
        "        \"engine\": \"google\"\n",
        "    }\n",
        "    print(f\"[Debug] SerpAPI Request to: https://serpapi.com/search with query: {query}\")\n",
        "\n",
        "    try:\n",
        "        # Make the API request with longer timeout\n",
        "        response = requests.get(\"https://serpapi.com/search\", params=params, timeout=15)\n",
        "        print(f\"[Debug] SerpAPI status code: {response.status_code}\")\n",
        "\n",
        "        # Handle HTTP errors\n",
        "        if response.status_code != 200:\n",
        "            print(f\"[Error] SerpAPI HTTP error: {response.status_code}\")\n",
        "            print(f\"Response text: {response.text}\")\n",
        "            return None, f\"SerpAPI error: HTTP {response.status_code}\"\n",
        "\n",
        "        # Parse the JSON response\n",
        "        try:\n",
        "            data = response.json()\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"[Error] Failed to parse JSON response: {response.text[:200]}...\")\n",
        "            return None, \"Failed to parse SerpAPI JSON response\"\n",
        "\n",
        "        # Debug the response structure\n",
        "        print(f\"[Debug] SerpAPI response keys: {list(data.keys())}\")\n",
        "\n",
        "        # Check for error messages in the API response\n",
        "        if \"error\" in data:\n",
        "            print(f\"[Error] SerpAPI error message: {data['error']}\")\n",
        "            return None, f\"SerpAPI error: {data['error']}\"\n",
        "\n",
        "        # Get organic search results\n",
        "        results = data.get(\"organic_results\", [])\n",
        "        if not results:\n",
        "            print(\"[Warning] No organic_results found in response\")\n",
        "            print(f\"Response preview: {debug_api_response(response.text)}\")\n",
        "            return None, \"No organic search results found\"\n",
        "\n",
        "        print(f\"[Success] Found {len(results)} organic results\")\n",
        "\n",
        "        # Process and clean the results\n",
        "        cleaned = []\n",
        "        for r in results:\n",
        "            title = r.get(\"title\", \"No title\")\n",
        "            link = r.get(\"link\", \"\")\n",
        "            snippet = r.get(\"snippet\", \"\")\n",
        "\n",
        "            # Try alternative fields if snippet is missing\n",
        "            if not snippet:\n",
        "                snippet = r.get(\"about_this_result\", {}).get(\"description\", \"\")\n",
        "\n",
        "            if link and snippet:  # Only include results with both link and snippet\n",
        "                cleaned.append((title, link, snippet))\n",
        "\n",
        "        if cleaned:\n",
        "            print(f\"[Success] Processed {len(cleaned)} valid results with snippets\")\n",
        "            # Return both the results and a success message\n",
        "            return cleaned, \"Success\"\n",
        "        else:\n",
        "            print(\"[Warning] No results with valid snippets found\")\n",
        "            return None, \"No results with valid snippets found\"\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\"[Error] SerpAPI request timed out\")\n",
        "        return None, \"SerpAPI request timed out\"\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"[Error] SerpAPI connection error\")\n",
        "        return None, \"SerpAPI connection error\"\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] Unexpected error: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None, f\"Unexpected error: {str(e)}\""
      ],
      "metadata": {
        "id": "PPVGXnP4__1L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "This function is the core of our research assistant's ability to gather information from the web. It uses the SerpAPI service to perform Google searches and retrieve results. The function is designed with:\n",
        "\n",
        "Robust error handling for various failure points (HTTP errors, JSON parsing, timeouts, connection errors)\n",
        "Extensive logging to help identify issues during development\n",
        "Data cleaning to extract only relevant information (title, link, snippet) from search results\n",
        "Fallback mechanisms to try alternative fields if the standard snippet is missing\n",
        "Quality control to ensure only results with both links and content snippets are included"
      ],
      "metadata": {
        "id": "dDK4AFu_w2hX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Analysis Functions**"
      ],
      "metadata": {
        "id": "DpUtSjVsACCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Text Processing Functions\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    \"\"\"Simple sentence tokenization without relying on NLTK\"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    return [s for s in sentences if s.strip()]\n",
        "\n",
        "def simple_extract_key_sentences(text, num_sentences=3):\n",
        "    \"\"\"Extract key sentences using word frequency without NLTK\"\"\"\n",
        "    sentences = simple_tokenize(text)\n",
        "    if not sentences or len(sentences) <= num_sentences:\n",
        "        return text\n",
        "\n",
        "    # Common English stopwords\n",
        "    stop_words = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
        "                      \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself',\n",
        "                      'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her',\n",
        "                      'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
        "                      'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n",
        "                      'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was',\n",
        "                      'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
        "                      'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
        "                      'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with',\n",
        "                      'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
        "                      'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
        "                      'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once'])\n",
        "\n",
        "    # Calculate word frequencies (excluding stopwords)\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    word_freq = Counter([word for word in words if word not in stop_words and len(word) > 1])\n",
        "\n",
        "    # Score sentences based on word frequencies\n",
        "    sentence_scores = {}\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        words_in_sentence = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
        "        sentence_scores[i] = sum(word_freq.get(word, 0) for word in words_in_sentence)\n",
        "\n",
        "    # Get indices of top sentences\n",
        "    top_indices = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]\n",
        "    top_indices = sorted([idx for idx, _ in top_indices])\n",
        "\n",
        "    # Return top sentences in original order\n",
        "    return ' '.join(sentences[idx] for idx in top_indices)"
      ],
      "metadata": {
        "id": "MRkuOMk5AImj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "These functions handle text processing for our research assistant, enabling it to break down text into sentences and identify the most important sentences based on word frequency. The implementation:\n",
        "\n",
        "Creates a custom sentence tokenizer using regular expressions instead of relying on NLTK\n",
        "\n",
        "Implements an extractive summarization algorithm based on word frequency\n",
        "\n",
        "Uses a predefined list of English stopwords to exclude common, low-information words\n",
        "\n",
        "Scores sentences based on the frequency of meaningful words they contain\n",
        "\n",
        "Returns the top N sentences (default: 3) in their original order to maintain text coherence"
      ],
      "metadata": {
        "id": "w8vef3eBxGZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarization Function**"
      ],
      "metadata": {
        "id": "1Z4aVS2EAMnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Search Results Summarization\n",
        "\n",
        "def summarize_search_results(search_results, query):\n",
        "    \"\"\"Create a summary from search results\"\"\"\n",
        "    if not search_results:\n",
        "        return \"No relevant information found.\"\n",
        "\n",
        "    # Create sections based on query terms\n",
        "    summary_parts = []\n",
        "\n",
        "    # Add introduction\n",
        "    summary_parts.append(f\"Based on the search results for '{query}', here's a summary of the key information:\")\n",
        "    summary_parts.append(\"\")\n",
        "\n",
        "    # Process each search result\n",
        "    for i, (title, url, snippet) in enumerate(search_results[:5]):  # Limit to top 5 results\n",
        "        # Extract key information\n",
        "        key_info = simple_extract_key_sentences(snippet)\n",
        "\n",
        "        # Add citation\n",
        "        result_summary = f\"According to {title} [Source {i+1}], {key_info}\"\n",
        "        summary_parts.append(result_summary)\n",
        "\n",
        "    # Add conclusion\n",
        "    summary_parts.append(\"\")\n",
        "    summary_parts.append(\"This summary is based on the available search results and may not represent a comprehensive analysis of the topic.\")\n",
        "\n",
        "    return \"\\n\".join(summary_parts)"
      ],
      "metadata": {
        "id": "IxUqqotiAO14"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "This function transforms raw search results into a coherent, readable summary with proper citations. The function:\n",
        "\n",
        "Takes a list of search results (from the search_google function) and the original query\n",
        "\n",
        "Creates an organized structure with an introduction, body, and conclusion\n",
        "\n",
        "Processes each search result (limiting to the top 5) to extract key information"
      ],
      "metadata": {
        "id": "ahdjIv0jxbYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PDF Report Generation**\n"
      ],
      "metadata": {
        "id": "SSuKhgobxsZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pdf(query, summary, sources):\n",
        "    \"\"\"\n",
        "    Generate a PDF report of the research results\n",
        "\n",
        "    Args:\n",
        "        query (str): The original research question\n",
        "        summary (str): The summary text with citations\n",
        "        sources (list): List of source URLs\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the generated PDF file\n",
        "    \"\"\"\n",
        "    # Create PDF object\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "\n",
        "    # Set title and metadata\n",
        "    pdf.set_title(f\"Research: {query}\")\n",
        "    pdf.set_author(\"AI Research Assistant\")\n",
        "\n",
        "    # Add header\n",
        "    pdf.set_font(\"Arial\", \"B\", 16)\n",
        "    pdf.cell(0, 10, \"AI Research Assistant Report\", ln=True, align=\"C\")\n",
        "    pdf.ln(5)\n",
        "\n",
        "    # Add query\n",
        "    pdf.set_font(\"Arial\", \"B\", 14)\n",
        "    pdf.cell(0, 10, \"Research Question:\", ln=True)\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "    pdf.multi_cell(0, 10, query)\n",
        "    pdf.ln(5)\n",
        "\n",
        "    # Add date\n",
        "    pdf.set_font(\"Arial\", \"I\", 10)\n",
        "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    pdf.cell(0, 6, f\"Report generated on: {current_date}\", ln=True)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Add summary\n",
        "    pdf.set_font(\"Arial\", \"B\", 14)\n",
        "    pdf.cell(0, 10, \"Research Summary:\", ln=True)\n",
        "    pdf.set_font(\"Arial\", \"\", 12)\n",
        "\n",
        "    # Process the summary by paragraph\n",
        "    paragraphs = summary.split('\\n')\n",
        "    for para in paragraphs:\n",
        "        if para.strip():  # Skip empty lines\n",
        "            pdf.multi_cell(0, 10, para)\n",
        "            pdf.ln(5)\n",
        "\n",
        "    # Add sources\n",
        "    if sources:\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", \"B\", 14)\n",
        "        pdf.cell(0, 10, \"Sources:\", ln=True)\n",
        "        pdf.set_font(\"Arial\", \"\", 12)\n",
        "\n",
        "        # Format sources as a list\n",
        "        for i, source in enumerate(sources):\n",
        "            if isinstance(source, str) and source.startswith('['):\n",
        "                pdf.multi_cell(0, 8, source)\n",
        "            else:\n",
        "                pdf.multi_cell(0, 8, f\"[{i+1}] {source}\")\n",
        "\n",
        "    # Set PDF output filename\n",
        "    filename = f\"research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
        "    output_path = filename\n",
        "\n",
        "    # Save the PDF\n",
        "    pdf.output(output_path)\n",
        "\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "VwpDDzCKxwH1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "This function creates a professional-looking PDF report from the research results, providing users with a downloadable document they can save or share. The function:\n",
        "\n",
        "Uses the FPDF library to create a structured report document\n",
        "\n",
        "Includes metadata like the research question, date, and time of generation\n",
        "\n",
        "Formats the summary text with appropriate paragraph breaks\n",
        "\n",
        "Creates a dedicated sources section with numbered references\n",
        "\n",
        "Generates a unique filename based on the current date and time\n",
        "\n",
        "Returns the file path to the generated PDF"
      ],
      "metadata": {
        "id": "BG3Jxuizx5-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Research Agent Function**"
      ],
      "metadata": {
        "id": "nrBs2cevAR7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Main Research Agent Function\n",
        "\n",
        "def research_agent_with_pdf(query, session_id=\"default-session\"):\n",
        "    \"\"\"Main research agent function with PDF generation\"\"\"\n",
        "    if not query.strip():\n",
        "        return \"Please enter a research question.\", [], None\n",
        "\n",
        "    print(f\"[Step 1] Processing query: {query}\")\n",
        "\n",
        "    print(\"[Step 2] Searching with SerpAPI...\")\n",
        "    search_results, status_message = search_google(query)\n",
        "\n",
        "    # Handle search failures\n",
        "    if search_results is None:\n",
        "        return f\"‚ùå Search failed: {status_message}\\n\\nPlease try again with a different query or check your SerpAPI key.\", [], None\n",
        "\n",
        "    print(\"[Step 3] Summarizing results...\")\n",
        "\n",
        "    # Create summary\n",
        "    try:\n",
        "        summary = summarize_search_results(search_results, query)\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] Summarization failed: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return f\" Summarization error: {str(e)}\", [], None\n",
        "\n",
        "    # Create numbered source links\n",
        "    display_links = [f\"[{i+1}] {result[1]}\" for i, result in enumerate(search_results)]\n",
        "\n",
        "    # Generate PDF\n",
        "    try:\n",
        "        print(\"[Step 4] Generating PDF report...\")\n",
        "        pdf_path = generate_pdf(query, summary, [f\"{result[0]}: {result[1]}\" for result in search_results])\n",
        "        print(f\"PDF generated: {pdf_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] PDF generation failed: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        pdf_path = None\n",
        "\n",
        "    return summary, display_links, pdf_path"
      ],
      "metadata": {
        "id": "juS7VvFSAVmz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "This is the main orchestration function for our research assistant, coordinating the entire process from query to final outputs. The function:\n",
        "\n",
        "Validates the input query to ensure it's not empty\n",
        "Follows a clear, step-by-step process with helpful logging:\n",
        "\n",
        "Step 1: Process the input query\n",
        "Step 2: Search for information using SerpAPI\n",
        "Step 3: Summarize the search results\n",
        "Step 4: Generate a PDF report\n",
        "\n",
        "\n",
        "Handles errors gracefully at each step, providing informative error messages\n",
        "Creates formatted source links for easy reference\n",
        "Returns three components: the summary text, a list of source links, and the path to the generated PDF\n",
        "\n",
        "This function ties together all the components we've built into a complete research assistant workflow, handling the entire process from the user's query to the delivery of organized research results."
      ],
      "metadata": {
        "id": "4AiKLka5yKiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **API Testing Function**"
      ],
      "metadata": {
        "id": "nTprE5xvAXz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: SerpAPI Testing Function\n",
        "\n",
        "def test_serpapi():\n",
        "    \"\"\"Test function to validate SerpAPI functionality\"\"\"\n",
        "    test_query = \"climate change impacts\"\n",
        "    print(f\"Testing SerpAPI with query: '{test_query}'\")\n",
        "\n",
        "    results, message = search_google(test_query)\n",
        "\n",
        "    if results:\n",
        "        print(f\"‚úÖ SerpAPI test successful: Found {len(results)} results\")\n",
        "        print(\"Sample result:\")\n",
        "        print(f\"Title: {results[0][0]}\")\n",
        "        print(f\"URL: {results[0][1]}\")\n",
        "        print(f\"Snippet: {results[0][2][:100]}...\")\n",
        "        return \"SerpAPI working correctly\"\n",
        "    else:\n",
        "        print(f\"‚ùå SerpAPI test failed: {message}\")\n",
        "        return f\"SerpAPI test failed: {message}\""
      ],
      "metadata": {
        "id": "ruucad74Abzj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "This utility function tests the SerpAPI connection to ensure that the research assistant can access web search functionality. The function:\n",
        "\n",
        "Runs a test query (\"climate change impacts\") to verify that SerpAPI is working\n",
        "\n",
        "Reports detailed results of the test, including sample output\n",
        "\n",
        "Returns a simple status message that can be displayed to the user\n",
        "\n",
        "This testing function is important because the entire research assistant depends on SerpAPI's functionality. Running this test early helps identify any API issues before the user tries to use the system."
      ],
      "metadata": {
        "id": "xyZkXhl8yTl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio User Interface**"
      ],
      "metadata": {
        "id": "w_uNgX1qAe53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface with PDF Download\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üß† AI Research Assistant\")\n",
        "    gr.Markdown(\"Enter a research question to get a summary with citations from web search results.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        test_button = gr.Button(\"Test SerpAPI Connection\")\n",
        "        api_status = gr.Textbox(label=\"API Status\", value=\"Click to test SerpAPI\")\n",
        "\n",
        "    # Input area\n",
        "    user_input = gr.Textbox(placeholder=\"Ask a research question...\", label=\"Your Question\")\n",
        "\n",
        "    # Output area\n",
        "    response_output = gr.Textbox(label=\"Summary with Citations\", lines=10)\n",
        "    links_output = gr.Textbox(label=\"Sources\", lines=6)\n",
        "\n",
        "    # PDF Download components\n",
        "    pdf_status = gr.Textbox(label=\"PDF Status\", visible=True)\n",
        "    pdf_output = gr.File(label=\"Download Research Report\", visible=True)\n",
        "\n",
        "    # Add a download button\n",
        "    download_button = gr.Button(\"Generate PDF Report\")\n",
        "\n",
        "    # Track the current results for PDF generation\n",
        "    current_query = gr.State(\"\")\n",
        "    current_summary = gr.State(\"\")\n",
        "    current_sources = gr.State([])\n",
        "\n",
        "    def handle_input(query):\n",
        "        try:\n",
        "            summary, links, pdf_path = research_agent_with_pdf(query)\n",
        "\n",
        "            # Store current results for later PDF generation\n",
        "            sources_list = links if isinstance(links, list) else links.split(\"\\n\")\n",
        "\n",
        "            if pdf_path and os.path.exists(pdf_path):\n",
        "                pdf_status_msg = \"‚úÖ PDF report generated successfully!\"\n",
        "                return summary, \"\\n\".join(sources_list) if sources_list else \"(No sources available)\", query, summary, sources_list, pdf_status_msg, pdf_path\n",
        "            else:\n",
        "                pdf_status_msg = \"‚ö†Ô∏è PDF generation failed. You can try the 'Generate PDF Report' button again.\"\n",
        "                return summary, \"\\n\".join(sources_list) if sources_list else \"(No sources available)\", query, summary, sources_list, pdf_status_msg, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] handle_input failed: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return f\"‚ùå Error: {str(e)}\", \"(Source loading failed)\", \"\", \"\", [], \"‚ùå PDF generation failed due to error in research process\", None\n",
        "\n",
        "    def generate_pdf_report(query, summary, sources):\n",
        "        \"\"\"Handler for the PDF download button\"\"\"\n",
        "        if not query or not summary:\n",
        "            return \"Please perform a search first before generating a PDF.\", None\n",
        "\n",
        "        try:\n",
        "            sources_list = sources if isinstance(sources, list) else sources.split(\"\\n\")\n",
        "            pdf_path = generate_pdf(query, summary, sources_list)\n",
        "\n",
        "            if os.path.exists(pdf_path):\n",
        "                return \"‚úÖ PDF report regenerated successfully!\", pdf_path\n",
        "            else:\n",
        "                return \" Failed to generate PDF report.\", None\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] PDF generation failed: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return f\" PDF generation error: {str(e)}\", None\n",
        "\n",
        "    # Connect test button\n",
        "    test_button.click(fn=test_serpapi, outputs=api_status)\n",
        "\n",
        "    # Connect main input\n",
        "    user_input.submit(\n",
        "        fn=handle_input,\n",
        "        inputs=user_input,\n",
        "        outputs=[response_output, links_output, current_query, current_summary, current_sources, pdf_status, pdf_output]\n",
        "    )\n",
        "\n",
        "    # Connect PDF download button\n",
        "    download_button.click(\n",
        "        fn=generate_pdf_report,\n",
        "        inputs=[current_query, current_summary, current_sources],\n",
        "        outputs=[pdf_status, pdf_output]\n",
        "    )"
      ],
      "metadata": {
        "id": "pbdDS1INAhKZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "This cell creates a user-friendly interface using Gradio, making our research assistant accessible through a web interface. The UI includes:\n",
        "\n",
        "A clear title and instructions for the user\n",
        "A test button to check the SerpAPI connection before use\n",
        "An input field for the research question\n",
        "Output areas for displaying the summary and source links\n",
        "Components for handling PDF generation and download\n",
        "State management to keep track of results for PDF regeneration\n",
        "Event handlers for:\n",
        "\n",
        "Testing the API connection\n",
        "Processing user input and displaying results\n",
        "Generating/regenerating PDF reports on demand"
      ],
      "metadata": {
        "id": "-7nT6nwMylrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing and Launch**"
      ],
      "metadata": {
        "id": "YmAOLSk2yoYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test SerpAPI before launching\n",
        "print(\"\\n===== TESTING SERPAPI CONNECTION =====\")\n",
        "test_result = test_serpapi()\n",
        "print(f\"Test result: {test_result}\")\n",
        "print(\"======================================\\n\")\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "-Qq7KYLTysjt",
        "outputId": "d11e1ce5-d75e-4d6d-ba81-0d52fbbd1a2e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== TESTING SERPAPI CONNECTION =====\n",
            "Testing SerpAPI with query: 'climate change impacts'\n",
            "[Debug] SerpAPI Request to: https://serpapi.com/search with query: climate change impacts\n",
            "[Debug] SerpAPI status code: 200\n",
            "[Debug] SerpAPI response keys: ['search_metadata', 'search_parameters', 'search_information', 'knowledge_graph', 'inline_images', 'inline_videos', 'related_questions', 'organic_results', 'top_stories', 'top_stories_link', 'top_stories_serpapi_link', 'related_searches', 'pagination', 'serpapi_pagination']\n",
            "[Success] Found 7 organic results\n",
            "[Success] Processed 7 valid results with snippets\n",
            "‚úÖ SerpAPI test successful: Found 7 results\n",
            "Sample result:\n",
            "Title: Climate change impacts\n",
            "URL: https://www.noaa.gov/education/resource-collections/climate/climate-change-impacts\n",
            "Snippet: Climate change affects the environment in many different ways, including rising temperatures, sea le...\n",
            "Test result: SerpAPI working correctly\n",
            "======================================\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://fc1ae6918789ce3902.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fc1ae6918789ce3902.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "The final cell runs a preliminary test of the SerpAPI connection and then launches the Gradio interface, making the research assistant accessible through a web browser. This cell:\n",
        "\n",
        "Runs a test of the SerpAPI connection to verify functionality before launching\n",
        "Displays the test results in the console\n",
        "Launches the Gradio interface with share=True, which creates a public URL that can be accessed from any device\n",
        "Makes the research assistant available for immediate use\n",
        "\n",
        "By testing before launch, we can identify any API issues early and ensure the system is ready for use. The share=True parameter creates a temporary public URL, making it easy to share the research assistant with others or access it from different devices.\n",
        "\n",
        "# **How to Use This Research Assistant**\n",
        "\n",
        "Setup: Run all cells in order to install dependencies, set up the environment, and launch the interface.\n",
        "Test Connection: Click the \"Test SerpAPI Connection\" button to verify that web search functionality is working.\n",
        "Research: Enter your research question in the input field and press Enter.\n",
        "Review: Read the summary and examine the sources provided.\n",
        "Download: Use the \"Generate PDF Report\" button to download a formatted PDF of your research results.\n",
        "\n",
        "This AI Research Assistant fulfills the requirements of Option 1 in the assignment by providing:\n",
        "\n",
        "Information retrieval and summarization from multiple sources\n",
        "Organization of findings into structured reports\n",
        "Citation management and reference tracking\n",
        "\n",
        "The implementation includes all required technical components:\n",
        "\n",
        "Agent architecture with input processing, reasoning, and output generation\n",
        "Tool integration (SerpAPI for web search, FPDF for document processing)\n",
        "Feedback mechanisms through the UI\n",
        "Safety measures with appropriate error handling and input validation"
      ],
      "metadata": {
        "id": "tMA3YBoky0XC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "This AI Research Assistant Agent demonstrates the implementation of a functional AI research tool that leverages web search capabilities to gather, analyze, and present information on user queries. The agent follows a ReAct (Reasoning and Acting) pattern by processing the user's input, retrieving relevant information, reasoning about the results, and then acting to produce a coherent summary.\n",
        "The agent successfully integrates external tools (SerpAPI for web search and text analysis for summarization), implements error handling for failed searches or summarization attempts, and provides a simple but effective user interface.\n",
        "Future improvements could include:\n",
        "\n",
        "Enhanced source credibility evaluation\n",
        "User feedback mechanisms to improve results\n",
        "Memory of past searches for context awareness\n",
        "More sophisticated text analysis and summarization techniques\n",
        "\n",
        "# **References & Citations**\n",
        "\n",
        "SerpAPI Documentation: https://serpapi.com/docs\n",
        "\n",
        "Gradio Documentation: https://gradio.app/docs"
      ],
      "metadata": {
        "id": "rBDXoXX1A9on"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTipas4vBJwi"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}